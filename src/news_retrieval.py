# ğŸ“¦ Built-in libraries
import re
import json
import urllib.parse
from datetime import datetime, timedelta
import os
import sys
from datetime import datetime
import nltk
# ğŸŒ Third-party libraries
import feedparser
from bs4 import BeautifulSoup
from transformers import pipeline
from newspaper import Article, ArticleException
import torch
import psutil  # ×ª×•×¡×™×£ ×œ×™×™×‘×•× ×× ×¢×“×™×™×Ÿ ××™×Ÿ
from nltk.tokenize import word_tokenize
import hashlib
import requests
from urllib.parse import urlparse
import html
import time
import urllib.parse

# Import from our modules
from config import logger, RSS_FEED_URL, rss_country_map
from text_processing import clean_text, clean_url, clean_title_for_matching, compute_text_hash, extract_source_from_url, extract_keywords
from json_handler import load_posted_news, load_skipped_news

#×¤×•× ×§×¦×™×•×ª ×œ×©×œ×™×¤×ª ×—×“×©×•×ª
#---------------------------------------------------------------------------------------------------------------------------------------------------------
#1
def get_google_alerts(time_range=1):
    """
    ×©×•×œ×£ ×›×ª×‘×•×ª ×-RSS ×©×”×•×’×“×¨×• ××¨××©.
    :param time_range: ××¡×¤×¨ ×”×™××™× ×œ××—×•×¨ ×œ×©×œ×™×¤×ª ×—×“×©×•×ª (×‘×¨×™×¨×ª ××—×“×œ: 1 - ×”×™×•× ×”× ×•×›×—×™)
    :return: ×¨×©×™××ª ×›×ª×‘×•×ª ×—×“×©×•×ª ×¢× ×©×“×•×ª × ×•×¡×¤×™×
    """
    articles = []
    today = datetime.today().date()
    start_date = today - timedelta(days=time_range)
    invalid_count = 0

    for rss_url in RSS_FEED_URL:
        try:
            response = requests.get(rss_url, timeout=10)
            response.raise_for_status()
            feed = feedparser.parse(response.text)

            if not feed.entries:
                print(f"âš ï¸ ××™×Ÿ ×›×ª×‘×•×ª ×‘-RSS: {rss_url}")
                continue

            print(f"ğŸ“¡ ××§×•×¨ RSS: {rss_url} - × ××¦××• {len(feed.entries)} ×›×ª×‘×•×ª.")
            rss_source = rss_country_map.get(rss_url, "Unknown")  # ×›××Ÿ ××•×¡×£ ×©×“×” ×”××“×™× ×”

            for entry in feed.entries:
                try:
                    article_id = entry.id if hasattr(entry, "id") else str(datetime.now().timestamp())
                    title = clean_text(entry.title) if hasattr(entry, "title") else None
                    raw_url = entry.link if hasattr(entry, "link") else None
                    clean_url = urllib.parse.parse_qs(urllib.parse.urlparse(raw_url).query).get("url", [raw_url])[0] if raw_url else None
                    summary = clean_text(entry.summary) if hasattr(entry, "summary") else ""
                    published_dt = datetime(*entry.published_parsed[:6]) if hasattr(entry, "published_parsed") else datetime.now()
                    published_date_obj = published_dt.date()
                    published_date = published_dt.strftime("%Y-%m-%d")
                    published_time = published_dt.strftime("%H:%M:%S")

                    # ×¡×™× ×•×Ÿ ×œ×¤×™ ×˜×•×•×— ×ª××¨×™×›×™×
                    if start_date <= published_date_obj <= today:
                        if not title or not clean_url:
                            print(f"âš ï¸ ×›×ª×‘×” ×œ× ×ª×§×™× ×” (×›×•×ª×¨×ª/URL ×—×¡×¨×™×) - ××“×œ×’.")
                            invalid_count += 1
                            continue

                        # ×‘×“×™×§×ª ××•×¨×š ×”×ª×§×¦×™×¨
                        word_count = len(summary.split())
                        if word_count < 10:
                            print(f"âš ï¸ ×ª×§×¦×™×¨ ×§×¦×¨ ××“×™ ({word_count} ××™×œ×™×) - ××“×œ×’.")
                            invalid_count += 1
                            continue

                        source = extract_source_from_url(clean_url)
                        keywords = extract_keywords(summary)
                        text_hash = compute_text_hash(summary) if summary.strip() else None

                        articles.append({
                            "id": article_id,
                            "title": title,
                            "url": clean_url,
                            "text_hash": text_hash,
                            "published_date": published_date,
                            "published_time": published_time,
                            "summary": summary,
                            "source": source,
                            "keywords": keywords,
                            "rss_source": rss_source  # ×”×•×¡×¤×ª ×”××“×™× ×”
                        })
                        print(f"âœ… × ×•×¡×¤×” ×›×ª×‘×”: {title}")

                except Exception as e:
                    print(f"âš ï¸ ×©×’×™××” ×‘×¢×™×‘×•×“ ×›×ª×‘×” ×-RSS ({rss_url}): {e}")

        except requests.RequestException as e:
            print(f"âŒ ×©×’×™××” ×‘×©×œ×™×¤×ª RSS ×-{rss_url}: {e}")

    print(f"ğŸ“¡ × ××¦××• {len(articles)} ×›×ª×‘×•×ª ×—×“×©×•×ª ××›×œ ×”-RSS (× ×“×—×•: {invalid_count}).")
    return articles




#2
def fetch_full_text(url, max_words=600):
    """
    ×©×œ×™×¤×ª ×˜×§×¡×˜ ××œ× ××›×ª×‘×”
    :param url: ×›×ª×•×‘×ª URL ×©×œ ×”××××¨
    :param max_words: ××¡×¤×¨ ×”××™×œ×™× ×”××§×¡×™××œ×™ ×œ×©×œ×™×¤×” (×‘×¨×™×¨×ª ××—×“×œ: 600)
    :return: ×˜×§×¡×˜ ×”××××¨ ××• ×”×•×“×¢×ª ×©×’×™××”
    """
    try:
        print(f"ğŸŒ ×× ×¡×” ×œ×©×œ×•×£ ×›×ª×‘×” ×-URL: {url}")
        # ×”×’×“×¨×ª ××××¨ ×¢× User-Agent ××•×ª××
        article = Article(url, language='en')
        article.download()
        print(f"â¬‡ï¸ ×”×•×¨×“×ª ×”××××¨ ×”×¦×œ×™×—×”: {url}")
        article.parse()
        print(f"ğŸ“ × ×™×ª×•×— ×”××××¨ ×”×¦×œ×™×—: {url}")

        text = article.text.strip()
        word_count = len(text.split())
        print(f"ğŸ“„ × ×©×œ×¤×• {word_count} ××™×œ×™× ××”×›×ª×‘×”: {url}")

        # ×‘×“×™×§×” ×× ×”×›×ª×‘×” ×§×¦×¨×” ××“×™
        if word_count < 10:
            print(f"âš ï¸ ×˜×§×¡×˜ ×§×¦×¨ ××“×™ (×¤×—×•×ª ×-10 ××™×œ×™×) - ×“×™×œ×•×’ ×¢×œ ×›×ª×‘×”: {url}")
            return "âš ï¸ ×˜×§×¡×˜ ×§×¦×¨ ××“×™ (×¤×—×•×ª ×-10 ××™×œ×™×)"

        # ×—×™×ª×•×š ×”×˜×§×¡×˜ ×‘××™×“×” ×•××¨×•×š ××“×™
        if word_count > max_words:
            text = " ".join(text.split()[:max_words])
            print(f"âœ‚ï¸ ×—×•×ª×š ××ª ×”×˜×§×¡×˜ ×œ-{max_words} ××™×œ×™×: {url}")

        print(f"âœ… ×˜×§×¡×˜ ×©×œ× × ×©×œ×£ ×‘×”×¦×œ×—×”: {url}")
        return text

    except ArticleException as ae:
        print(f"âš ï¸ ×©×’×™××” ×‘×¢×™×‘×•×“ ××××¨ (ArticleException) ×-{url}: {ae}")
        return "âš ï¸ ×©×’×™××ª ×¢×™×‘×•×“ ××××¨"

    except ConnectionError as ce:
        print(f"âš ï¸ ×©×’×™××ª ×—×™×‘×•×¨ ×œ×›×ª×‘×” ×-{url}: {ce}")
        return "âš ï¸ ×©×’×™××ª ×—×™×‘×•×¨ ×œ××ª×¨"

    except Exception as e:
        print(f"âš ï¸ ×©×’×™××” ×›×œ×œ×™×ª ×‘×©×œ×™×¤×ª ×›×ª×‘×” ×-{url}: {e}")
        return "âš ï¸ ×©×’×™××” ×›×œ×œ×™×ª ×‘×©×œ×™×¤×ª ×›×ª×‘×”"


#3
def filter_new_articles(articles):
    try:
        posted_news = load_posted_news()
        print(f"âœ… × ×ª×•× ×™ ×›×ª×‘×•×ª ×©×›×‘×¨ × ×©×œ×—×• × ×˜×¢× ×• ×‘×”×¦×œ×—×”. ({len(posted_news)} ×¤×¨×™×˜×™×)")
    except Exception as e:
        print(f"âš ï¸ ×©×’×™××” ×‘×˜×¢×™× ×ª ×›×ª×‘×•×ª ×©× ×©×œ×—×•: {e}")
        posted_news = []

    try:
        skipped_news = load_skipped_news()
        print(f"âœ… × ×ª×•× ×™ ×›×ª×‘×•×ª ×©× ×›×©×œ×• × ×˜×¢× ×• ×‘×”×¦×œ×—×”. ({len(skipped_news)} ×¤×¨×™×˜×™×)")
    except Exception as e:
        print(f"âš ï¸ ×©×’×™××” ×‘×˜×¢×™× ×ª ×›×ª×‘×•×ª ×©× ×›×©×œ×•: {e}")
        skipped_news = {}

    processed_titles = set()
    processed_urls = set()
    processed_hashes = set()

    for item in posted_news:
        processed_titles.add(clean_title_for_matching(item.get("title", "")))
        processed_urls.add(clean_url(item.get("url", "")))
        processed_hashes.add(item.get("text_hash", ""))

    for item in skipped_news.values():
        processed_titles.add(clean_title_for_matching(item.get("title", "")))
        processed_urls.add(clean_url(item.get("url", "")))
        processed_hashes.add(item.get("text_hash", ""))

    new_articles = []
    duplicate_count = 0

    for article in articles:
        title = clean_title_for_matching(article.get("title", ""))
        url = clean_url(article.get("url", ""))
        content = article.get("summary", "")
        content_word_count = len(content.split())

        print(f"[DEBUG] ×‘×•×“×§ ×›×ª×‘×”: title='{title}' | url='{url}' | ××™×œ×™× ×‘×ª×§×¦×™×¨={content_word_count}")

        if not title or not url:
            print(f"âš ï¸ ×›×ª×‘×” ×œ×œ× ×›×•×ª×¨×ª ××• ×›×ª×•×‘×ª: {article}")
            continue

        if title in processed_titles:
            duplicate_count += 1
            print(f"[ğŸ” DUPLICATE_TITLE] '{title}' × ××¦× ×›×‘×¨ â€“ ×“×™×œ×•×’.")
            continue

        if url in processed_urls:
            duplicate_count += 1
            print(f"[ğŸ” DUPLICATE_URL] '{url}' × ××¦× ×›×‘×¨ â€“ ×“×™×œ×•×’.")
            continue

        # ×—×™×©×•×‘ hash ×ª××™×“, ×× ×™×© ×ª×§×¦×™×¨ ×›×œ×©×”×•
        text_hash = compute_text_hash(content) if content.strip() else None
        if text_hash:
            print(f"[HASH] ×—×•×©×‘ hash: {text_hash}")
            if text_hash in processed_hashes:
                duplicate_count += 1
                print(f"[ğŸ” DUPLICATE_HASH] hash='{text_hash}' ×›×‘×¨ ×§×™×™× â€“ ×“×™×œ×•×’.")
                continue

        article["text_hash"] = text_hash
        new_articles.append(article)
        processed_titles.add(title)
        processed_urls.add(url)
        if text_hash:
            processed_hashes.add(text_hash)

    print(f"ğŸ§ ××¡× ×Ÿ {duplicate_count} ×›×ª×‘×•×ª ×›×¤×•×œ×•×ª ××• ×©×›×‘×¨ × ×›×©×œ×• ×‘×¢×‘×¨.")
    print(f"âœ… × ××¦××• {len(new_articles)} ×›×ª×‘×•×ª ×—×“×©×•×ª ×œ×©×œ×™×—×”.")
    return new_articles
